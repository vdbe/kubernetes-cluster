---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 5m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.9.9
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system
  install:
    createNamespace: true
    remediation: # perform remediation when helm install fails
      retries: 5
  upgrade:
    remediation: # perform remediation when helm upgrade fails
      retries: 5
      remediateLastFailure: true # remediate the last failure, when no retries remain
    cleanupOnFail: true
  dependsOn:
    - name: rook-ceph
      namespace: rook-ceph
  values:
    toolbox:
      enabled: true

    #monitoring:
    #  enabled: true
    #  #createPrometheusRules: true

    #configOverride: |
    #  [global]
    #  bdev_enable_discard = true
    #  bdev_async_discard = true

    #cephClusterSpec:
    #  network:
    #    provider: host

    #  resources:
    #    mgr:
    #      limits:
    #        memory: "1Gi"
    #      requests:
    #        cpu: "500m"
    #        memory: "512Mi"
    #    mon:
    #      limits:
    #        memory: "2Gi"
    #      requests:
    #        cpu: "1000m"
    #        memory: "1Gi"
    #    osd:
    #      limits:
    #        memory: "4Gi"
    #      requests:
    #        cpu: "1000m"
    #        memory: "4Gi"
    #    prepareosd:
    #      limits:
    #        memory: "400Mi"
    #      requests:
    #        cpu: "500m"
    #        memory: "50Mi"
    #    mgr-sidecar:
    #      limits:
    #        memory: "100Mi"
    #      requests:
    #        cpu: "100m"
    #        memory: "40Mi"
    #    crashcollector:
    #      limits:
    #        memory: "60Mi"
    #      requests:
    #        cpu: "100m"
    #        memory: "60Mi"
    #    logcollector:
    #      limits:
    #        memory: "1Gi"
    #      requests:
    #        cpu: "100m"
    #        memory: "100Mi"
    #    cleanup:
    #      limits:
    #        memory: "1Gi"
    #      requests:
    #        cpu: "500m"
    #        memory: "100Mi"
    #  crashCollector:
    #    disable: false

    #  dashboard:
    #    enabled: true
    #    urlPrefix: /

      storage:
        useAllNodes: false
        useAllDevices: false
        config:
          osdsPerDevice: "1"
        nodes:
          - name: "k3s-node-01"
            devices:
              - name: "xvdb"
          - name: "k3s-node-02"
            devices:
              - name: "xvdb"
          - name: "k3s-node-03"
            devices:
              - name: "xvdb"
        onlyApplyOSDPlacement: false

      # The section for configuring management of daemon disruptions during upgrade or fencing.
      disruptionManagement:
        # If true, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically
        # via the strategy outlined in the [design](https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md). The operator will
        # block eviction of OSDs by default and unblock them safely when drains are detected.
        managePodBudgets: true
        # A duration in minutes that determines how long an entire failureDomain like `region/zone/host` will be held in `noout` (in addition to the
        # default DOWN/OUT interval) when it is draining. This is only relevant when  `managePodBudgets` is `true`. The default value is `30` minutes.
        osdMaintenanceTimeout: 30
        # A duration in minutes that the operator will wait for the placement groups to become healthy (active+clean) after a drain was completed and OSDs came back up.
        # Operator will continue with the next drain if the timeout exceeds. It only works if `managePodBudgets` is `true`.
        # No values or 0 means that the operator will wait until the placement groups are healthy before unblocking the next drain.
        pgHealthCheckTimeout: 0
        # If true, the operator will create and manage MachineDisruptionBudgets to ensure OSDs are only fenced when the cluster is healthy.
        # Only available on OpenShift.
        manageMachineDisruptionBudgets: false
        # Namespace in which to watch for the MachineDisruptionBudgets.
        machineDisruptionBudgetNamespace: openshift-machine-api

    #  placement:
    #    osd:
    #      tolerations:
    #      - effect: NoExecute
    #        operator: Exists
    #      - effect: NoSchedule
    #        operator: Exists
    #      - effect: PreferNoSchedule
    #        operator: Exists
    # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
    # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
    # tolerate taints with a key of 'storage-node'.
    # placement:
    #   all:
    #     nodeAffinity:
    #       requiredDuringSchedulingIgnoredDuringExecution:
    #         nodeSelectorTerms:
    #           - matchExpressions:
    #             - key: role
    #               operator: In
    #               values:
    #               - storage-node
    #     podAffinity:
    #     podAntiAffinity:
    #     topologySpreadConstraints:
    #     tolerations:
    #     - key: storage-node
    #       operator: Exists
    #   # The above placement information can also be specified for mon, osd, and mgr components
    #   mon:
    #   # Monitor deployments may contain an anti-affinity rule for avoiding monitor
    #   # collocation on the same node. This is a required rule when host network is used
    #   # or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
    #   # preferred rule with weight: 50.
    #   osd:
    #   mgr:
    #   cleanup:


    cephBlockPoolsVolumeSnapshotClass:
      enabled: false

    cephBlockPools:
      - name: ceph-blockpool
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ceph-block
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

    #cephFileSystems:
    #  - name: ceph-filesystem
    #    spec:
    #      metadataPool:
    #        replicated:
    #          size: 3
    #      dataPools:
    #        - failureDomain: host
    #          replicated:
    #            size: 3
    #      metadataServer:
    #        activeCount: 1
    #        activeStandby: true
    #        resources:
    #          limits:
    #            # cpu: "500m"
    #            memory: "4Gi"
    #          requests:
    #            cpu: "1000m"
    #            memory: "4Gi"
    #    storageClass:
    #      enabled: true
    #      isDefault: false
    #      name: ceph-filesystem
    #      reclaimPolicy: Delete
    #      allowVolumeExpansion: true
    #      mountOptions: []
    #      parameters:
    #        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    #        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    #        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    #        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    #        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    #        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
    #        csi.storage.k8s.io/fstype: ext4

    cephFileSystems: []
    cephObjectStores: []

  # healthChecks
  # Valid values for daemons are 'mon', 'osd', 'status'
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    # Change pod liveness probe timing or threshold values. Works for all mon,mgr,osd daemons.
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
    # Change pod startup probe timing or threshold values. Works for all mon,mgr,osd daemons.
    startupProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
