---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 5m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.9.9
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system
  install:
    createNamespace: true
    remediation: # perform remediation when helm install fails
      retries: 5
  upgrade:
    remediation: # perform remediation when helm upgrade fails
      retries: 5
      remediateLastFailure: true # remediate the last failure, when no retries remain
    cleanupOnFail: true
  dependsOn:
    - name: rook-ceph
      namespace: rook-ceph
  values:
    toolbox:
      enabled: true

    #monitoring:
    #  enabled: true
    #  #createPrometheusRules: true

    #configOverride: |
    #  [global]
    #  bdev_enable_discard = true
    #  bdev_async_discard = true

    #cephClusterSpec:
    #  network:
    #    provider: host

    #  resources:
    #    mgr:
    #      limits:
    #        memory: "1Gi"
    #      requests:
    #        cpu: "500m"
    #        memory: "512Mi"
    #    mon:
    #      limits:
    #        memory: "2Gi"
    #      requests:
    #        cpu: "1000m"
    #        memory: "1Gi"
    #    osd:
    #      limits:
    #        memory: "4Gi"
    #      requests:
    #        cpu: "1000m"
    #        memory: "4Gi"
    #    prepareosd:
    #      limits:
    #        memory: "400Mi"
    #      requests:
    #        cpu: "500m"
    #        memory: "50Mi"
    #    mgr-sidecar:
    #      limits:
    #        memory: "100Mi"
    #      requests:
    #        cpu: "100m"
    #        memory: "40Mi"
    #    crashcollector:
    #      limits:
    #        memory: "60Mi"
    #      requests:
    #        cpu: "100m"
    #        memory: "60Mi"
    #    logcollector:
    #      limits:
    #        memory: "1Gi"
    #      requests:
    #        cpu: "100m"
    #        memory: "100Mi"
    #    cleanup:
    #      limits:
    #        memory: "1Gi"
    #      requests:
    #        cpu: "500m"
    #        memory: "100Mi"
    #  crashCollector:
    #    disable: false

    #  dashboard:
    #    enabled: true
    #    urlPrefix: /

      storage:
        useAllNodes: false
        useAllDevices: false
        config:
          osdsPerDevice: "1"
        nodes:
          - name: "k3s-node-01"
            devices:
              - name: "xvdb"
          - name: "k3s-node-02"
            devices:
              - name: "xvdb"
          - name: "k3s-node-03"
            devices:
              - name: "xvdb"
        onlyApplyOSDPlacement: false


    #  placement:
    #    osd:
    #      tolerations:
    #      - effect: NoExecute
    #        operator: Exists
    #      - effect: NoSchedule
    #        operator: Exists
    #      - effect: PreferNoSchedule
    #        operator: Exists
    # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
    # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
    # tolerate taints with a key of 'storage-node'.
    # placement:
    #   all:
    #     nodeAffinity:
    #       requiredDuringSchedulingIgnoredDuringExecution:
    #         nodeSelectorTerms:
    #           - matchExpressions:
    #             - key: role
    #               operator: In
    #               values:
    #               - storage-node
    #     podAffinity:
    #     podAntiAffinity:
    #     topologySpreadConstraints:
    #     tolerations:
    #     - key: storage-node
    #       operator: Exists
    #   # The above placement information can also be specified for mon, osd, and mgr components
    #   mon:
    #   # Monitor deployments may contain an anti-affinity rule for avoiding monitor
    #   # collocation on the same node. This is a required rule when host network is used
    #   # or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
    #   # preferred rule with weight: 50.
    #   osd:
    #   mgr:
    #   cleanup:


    cephBlockPoolsVolumeSnapshotClass:
      enabled: false

    cephBlockPools:
      - name: ceph-blockpool
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ceph-block
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

    #cephFileSystems:
    #  - name: ceph-filesystem
    #    spec:
    #      metadataPool:
    #        replicated:
    #          size: 3
    #      dataPools:
    #        - failureDomain: host
    #          replicated:
    #            size: 3
    #      metadataServer:
    #        activeCount: 1
    #        activeStandby: true
    #        resources:
    #          limits:
    #            # cpu: "500m"
    #            memory: "4Gi"
    #          requests:
    #            cpu: "1000m"
    #            memory: "4Gi"
    #    storageClass:
    #      enabled: true
    #      isDefault: false
    #      name: ceph-filesystem
    #      reclaimPolicy: Delete
    #      allowVolumeExpansion: true
    #      mountOptions: []
    #      parameters:
    #        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    #        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    #        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    #        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    #        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    #        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
    #        csi.storage.k8s.io/fstype: ext4

    cephFileSystems: []
    cephObjectStores: []
